{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups_folder_path = '/content/drive/My Drive/beans'\n",
    "# categories = [\"broken\", \"insect\", \"normal\"] # 해당 이미지들이 들어가 있는 폴더명 넣어주기 \n",
    "# num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting sklearn\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\nCollecting scikit-learn\n  Downloading scikit_learn-0.23.0-cp37-cp37m-win_amd64.whl (6.8 MB)\nCollecting joblib>=0.11\n  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\nRequirement already satisfied: scipy>=0.19.1 in c:\\users\\admin\\anaconda3\\envs\\project_opencv\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\nRequirement already satisfied: numpy>=1.13.3 in c:\\users\\admin\\anaconda3\\envs\\project_opencv\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\nCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-2.0.0-py3-none-any.whl (34 kB)\nBuilding wheels for collected packages: sklearn\n  Building wheel for sklearn (setup.py): started\n  Building wheel for sklearn (setup.py): finished with status 'done'\n  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1320 sha256=039a5a6d34665b365cd6d63473fee7a168aa3d9d3535ec647a162543acf0dc02\n  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\46\\ef\\c3\\157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\nSuccessfully built sklearn\nInstalling collected packages: joblib, threadpoolctl, scikit-learn, sklearn\nSuccessfully installed joblib-0.15.1 scikit-learn-0.23.0 sklearn-0.0 threadpoolctl-2.0.0\nNote: you may need to restart the kernel to use updated packages.\n"
    }
   ],
   "source": [
    "#pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import os \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import gridfs \n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_folder_path = './normal_data'\n",
    "categories = [\"normal_align_test_image\", \"normal_processed_data\", \"normal_raw_data\" ] # 해당 이미지들이 들어가 있는 폴더명 넣어주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mongodb에서 사진 불러오기 \n",
    "# # category 나눠서 들어갈 수 있도록 하기 \n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__' :\n",
    "    \n",
    "#     # connect to database \n",
    "#     client = MongoClient('127.0.0.1', 27017)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     # read in the image \n",
    "\n",
    "#     for i,d in enumerate(categories):\n",
    "#         files = os.listdir(groups_folder_path + '/'+ d)\n",
    "#         database = client[d]\n",
    "\n",
    "#         # create a new gridfs object\n",
    "#         fs = gridfs.GridFS(database)\n",
    "\n",
    "#         for f in files : \n",
    "#             img = open(groups_folder_path + '/'+ d +'/' + f, 'rb')\n",
    "#             thedata = img.read()\n",
    "#             # store the data in the database. \n",
    "#             # Returns the id of the file in gridFS \n",
    "#             stored = fs.put(thedata, filename = f)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     # retrieve what was just stored \n",
    "#     #outputdata = fs.get(stored).read()\n",
    "\n",
    "#     # create an output file and store the image in the output file \n",
    "#     # outfilename = './OpenCV/data/20200508_test1_sample1.jpg'\n",
    "#     # output = open(outfilename, 'wb')\n",
    "#     # output.write(outputdata)\n",
    "#     # # close the output file \n",
    "#     # output.close()\n",
    "\n",
    "#     # for experimental code restore to known state and close connection \n",
    "\n",
    "#     #fs.delete(stored)\n",
    "#     #client.drop_database('example')\n",
    "#     client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Note: you may need to restart the kernel to use updated packages.\nERROR: Could not find a version that satisfies the requirement StringIO (from versions: none)\nERROR: No matching distribution found for StringIO\n"
    }
   ],
   "source": [
    "# pip install StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2160, 2160)"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "# img = Image.open(\"./normal_data/normal_align_test_image/20200518_test_inadvance1_off.jpeg\")\n",
    "# img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(28, 1)"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "data = []\n",
    "label = []\n",
    "\n",
    "# groups_folder_path = './normal_data'\n",
    "# categories = [\"normal_align_test_image\", \"normal_processed_data\", \"normal_raw_data\" ] # 해당 이미지들이 들어가 있는 폴더명 넣어주기 \n",
    "\n",
    "if __name__ == '__main__' :  \n",
    "  client = MongoClient('127.0.0.1', 27017)\n",
    "  # read in the image \n",
    "  for i,d in enumerate(categories):\n",
    "    database  = client[d]\n",
    "    files     = os.listdir(groups_folder_path + '/'+ d)\n",
    "    \n",
    "    # create a new gridfs object\n",
    "    fs        = gridfs.GridFS(database)\n",
    "\n",
    "    #print( '->' +  d )\n",
    "    for f in files : \n",
    "      #print( groups_folder_path + '/'+ d +'/' + f )\n",
    "      fp         = open(groups_folder_path + '/'+ d +'/' + f, 'rb')\n",
    "      # bytes로 읽는다\n",
    "      thedata     = fp.read()            \n",
    "      \n",
    "      image = Image.open(io.BytesIO(thedata))\n",
    "      #image.show()\n",
    "      \n",
    "      \n",
    "      \n",
    "      #print( type(thedata), len(thedata) )\n",
    "      # 디비에 저장\n",
    "      stored      = fs.put(thedata, filename = f)      \n",
    "      # 디비에서 다시 읽는다\n",
    "      outputdata  = fs.get(stored).read()\n",
    "      #print( type(outputdata), len(outputdata) )\n",
    "      #print( '-'*10 )\n",
    "      \n",
    "      thedata2 = Image.open(fp, mode='r')\n",
    "      \n",
    "\n",
    "      #thedata2 = Image.frombytes('RGB', (2160,2160), outputdata, 'raw')      \n",
    "      # #thedata = Image.open(StringIO.StringIO(outputdata))\n",
    "      # # 이미지 흑백으로 변경 \n",
    "      img = thedata2.convert('L')\n",
    "      # # 이미지를 28, 28로 일괄 리사이즈 \n",
    "      resize_img = img.resize((28,28))\n",
    "      black_resize_img = np.asarray(resize_img)\n",
    "      # # 가공한 이미지 추가 \n",
    "      data.append(black_resize_img)\n",
    "      # # label : broken, insect, normal\n",
    "      label.append(i)\n",
    "\n",
    "      fp.close()\n",
    "      #break\n",
    "\n",
    "\n",
    "  client.close()\n",
    "\n",
    "pd.DataFrame(data[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype = 'float32')\n",
    "label = np.array(label, dtype = 'int64')\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(data, label, test_size = 0.1)\n",
    "\n",
    "train_X = torch.from_numpy(train_X).float()\n",
    "train_Y = torch.from_numpy(train_Y).long()\n",
    "\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_Y = torch.from_numpy(test_Y).long()\n",
    "\n",
    "#train = TensorDataset(train_X, train_Y)\n",
    "#train_loader = DataLoader(train, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[212., 212., 212.,  ..., 213., 213., 213.],\n         [212., 212., 212.,  ..., 213., 213., 213.],\n         [212., 212., 212.,  ..., 213., 213., 213.],\n         ...,\n         [214., 214., 214.,  ..., 214., 214., 214.],\n         [214., 214., 214.,  ..., 214., 214., 214.],\n         [214., 214., 214.,  ..., 214., 214., 214.]],\n\n        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n\n        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n\n        ...,\n\n        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n\n        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n\n        [[216., 216., 216.,  ..., 213., 213., 213.],\n         [216., 216., 216.,  ..., 213., 213., 213.],\n         [216., 216., 216.,  ..., 213., 213., 213.],\n         ...,\n         [219., 215., 213.,  ..., 210., 216., 185.],\n         [226., 215., 215.,  ..., 210., 214., 177.],\n         [225., 216., 218.,  ..., 212., 215., 174.]]])"
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([2, 1, 1,  ..., 1, 1, 2])"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[211., 211., 211.,  ..., 213., 212., 211.],\n         [211., 211., 211.,  ..., 213., 212., 211.],\n         [211., 211., 211.,  ..., 213., 212., 211.],\n         ...,\n         [210., 210., 210.,  ..., 211., 212., 211.],\n         [210., 210., 210.,  ..., 211., 212., 211.],\n         [210., 210., 210.,  ..., 211., 212., 211.]],\n\n        [[209., 209., 209.,  ..., 209., 209., 209.],\n         [211., 210., 209.,  ..., 209., 209., 209.],\n         [204., 207., 210.,  ..., 209., 209., 209.],\n         ...,\n         [210., 210., 210.,  ..., 207., 206., 206.],\n         [210., 210., 210.,  ..., 207., 206., 206.],\n         [210., 210., 210.,  ..., 207., 206., 206.]],\n\n        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n\n        ...,\n\n        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n\n        [[204., 206., 205.,  ..., 207., 207., 207.],\n         [205., 205., 205.,  ..., 207., 207., 207.],\n         [205., 204., 205.,  ..., 207., 207., 207.],\n         ...,\n         [203., 203., 203.,  ..., 207., 207., 207.],\n         [204., 204., 204.,  ..., 208., 208., 208.],\n         [205., 205., 205.,  ..., 209., 209., 209.]],\n\n        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]])"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 0, 1, 2, 1, 1, 1, 1,\n        1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1,\n        1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1,\n        2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2,\n        2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2,\n        1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2,\n        2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1,\n        2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1,\n        1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2,\n        1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1,\n        1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2,\n        1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1,\n        1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2,\n        1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1,\n        2, 1, 2, 1])"
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3056, 28, 28])"
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3056])"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([340, 28, 28])"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([340])"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 차원 맞추기\n",
    "train_X = train_X.unsqueeze(1)\n",
    "test_X = test_X.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3056, 1, 28, 28])"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([340, 1, 28, 28])"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 GPU를 사용 가능하다면 device 값이 cuda가 되고, 아니라면 cpu가 됩니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.utils.data.dataset.TensorDataset"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "dataset = TensorDataset(train_X, train_Y)\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 첫번째층\n",
    "        # ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 두번째층\n",
    "        # ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
    "\n",
    "        # 전결합층 한정으로 가중치 초기화\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "총 배치의 수 : 30\n[Epoch:    1] cost = 0.112832822\n[Epoch:    2] cost = 0.0142681515\n[Epoch:    3] cost = 0.0116213951\n[Epoch:    4] cost = 0.0120147942\n[Epoch:    5] cost = 0.0196542889\n[Epoch:    6] cost = 0.00492227357\n[Epoch:    7] cost = 0.00171656837\n[Epoch:    8] cost = 0.000686642074\n[Epoch:    9] cost = 0.000442401128\n[Epoch:   10] cost = 0.000278438238\n[Epoch:   11] cost = 0.000239286892\n[Epoch:   12] cost = 0.000175217836\n[Epoch:   13] cost = 0.000151890839\n[Epoch:   14] cost = 0.000131074761\n[Epoch:   15] cost = 0.000115670962\n"
    }
   ],
   "source": [
    "# 모델을 정의합니다.\n",
    "\n",
    "# CNN 모델 정의\n",
    "model = CNN().to(device)\n",
    "# 비용 함수와 옵티마이저를 정의합니다.\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 총 배치의 수를 출력해보겠습니다.\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))\n",
    "# 총 배치의 수 : 600\n",
    "# 총 배치의 수는 600입니다. 그런데 배치 크기를 100으로 했으므로 결국 훈련 데이터는 총 60,000개란 의미입니다. 이제 모델을 훈련시켜보겠습니다. (시간이 꽤 오래 걸립니다.)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y는 레이블.\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 예측 하기 \n",
    "# 참고 사이트 \n",
    "\"\"\"\n",
    "https://github.com/MLlounge/ML-Project/blob/master/Basic%20Project/CNN%20-%20Distinguish%20language%20according%20to%20characters/CNN%20-%20Distinguish%20language%20according%20to%20characters.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './normal_data/normal_align_test_image/20200518_test_inadvance3.jpeg'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-38aee76b32d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# read image and image preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mpre_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./normal_data/normal_align_test_image/20200518_test_inadvance3.jpeg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mpre_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# change into black-and-white image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mpre_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# resize image to 32 * 32 * 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_opencv\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2843\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './normal_data/normal_align_test_image/20200518_test_inadvance3.jpeg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "\n",
    "\n",
    "# img = Image.open(\"./normal_data/normal_align_test_image/20200518_test_inadvance1_off.jpeg\")\n",
    "# img.size\n",
    "\n",
    "#  # # 이미지 흑백으로 변경 \n",
    "#       img = thedata2.convert('L')\n",
    "#       # # 이미지를 28, 28로 일괄 리사이즈 \n",
    "#       resize_img = img.resize((28,28))\n",
    "#       black_resize_img = np.asarray(resize_img)\n",
    "#       # # 가공한 이미지 추가 \n",
    "#       data.append(black_resize_img)\n",
    "#       # # label : broken, insect, normal\n",
    "#       label.append(i)\n",
    "\n",
    "#       fp.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read image and image preprocessing \n",
    "pre_img = Image.open(\"./normal_data/normal_align_test_image/20200518_test_inadvance3.jpeg\")\n",
    "pre_img = pre_img.convert(\"L\") # change into black-and-white image\n",
    "pre_img = pre_img.resize((28,28)) # resize image to 32 * 32 * 1 \n",
    "pre_data = np.asarray(pre_img) # np type \n",
    "#pre_data = 1 * (pre_data > np.mean(pre_data)) # change to binary data \n",
    "#pre_data = np.reshape(pre_data, [-1, 28, 28, 1])\n",
    "type(pre_data), pre_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction \n",
    "pre_result = model.predict(pre_data)\n",
    "\n",
    "print(pre_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitprojectopencvconda6fc4edbc446e42ca82ff567e09c1586a",
   "display_name": "Python 3.7.7 64-bit ('project_opencv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}