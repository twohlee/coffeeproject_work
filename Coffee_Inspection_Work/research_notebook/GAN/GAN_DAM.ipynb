{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN_DAM","provenance":[],"collapsed_sections":[],"mount_file_id":"1SRIK7zdjEHQ94yTMiS71nhWacnv0zPbq","authorship_tag":"ABX9TyOb/6blFSr280C4JI+E0eYF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0QE72hqtsrmD","colab_type":"text"},"source":["## 실행 코드"]},{"cell_type":"markdown","metadata":{"id":"lYzDYz7aszh9","colab_type":"text"},"source":["### 0. import library"]},{"cell_type":"code","metadata":{"id":"VNIJdkwkr-Xs","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544B_o9cyw8L","colab_type":"text"},"source":["#### GPU check"]},{"cell_type":"code","metadata":{"id":"ZPf31uinyxTK","colab_type":"code","colab":{}},"source":["import os\n","import imageio\n","\n","if torch.cuda.is_available():\n","    use_gpu = True\n","else:\n","    use_gpu = False\n","leave_log = True\n","if leave_log:\n","    result_dir = '/content/drive/My Drive/Colab deeplearning/GANDAM_bean'\n","    if not os.path.isdir(result_dir):\n","        os.mkdir(result_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"171OZZV7yxRK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leIUnb9is-cN","colab_type":"text"},"source":["### 1. 데이터 로드 & 전처리 방식 지정"]},{"cell_type":"code","metadata":{"id":"r61HKPNInd5B","colab_type":"code","colab":{}},"source":["#데이터 전처리 방식을 지정한다.\n","transform = transforms.Compose([\n","  transforms.ToTensor(), # 데이터를 파이토치의 Tensor 형식으로바꾼다.\n","  transforms.Normalize(mean=(0.5,), std=(0.5,)) # 픽셀값 0 ~ 1 -> -1 ~ 1\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMfPc1lINmaN","colab_type":"code","colab":{}},"source":["import glob\n","import os\n","import sys\n","\n","dataPath = \"/content/drive/My Drive/bean data\"\n","norm_imgs = glob.glob(os.path.join(dataPath+\"/normal\",'test_*'))\n","defect_imgs = glob.glob(os.path.join(dataPath+\"/defect\",'test_*'))\n","imgs = norm_imgs + defect_imgs\n","# imgs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GBdIQtwTYo2","colab_type":"code","colab":{}},"source":["dataPath = \"/content/drive/My Drive/bean data\"\n","after_imgs = glob.glob(os.path.join(dataPath+\"/after\",'test_*'))\n","imgs = after_imgs\n","# imgs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t0DjH21NYEF3","colab":{}},"source":["dataPath = \"/content/drive/My Drive/bean data\"\n","after_imgs = glob.glob(os.path.join(dataPath+\"/after_after\",'test_*'))\n","# defect_imgs = glob.glob(os.path.join(dataPath+\"/defect\",'test_*'))\n","# imgs = after_imgs + defect_imgs\n","imgs = after_imgs\n","# imgs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rq1XC9vdMeh","colab_type":"text"},"source":["<hr>\n","\n","# 코드 정리하기!!!!!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KBwnwkB5NnGF","colab":{}},"source":["from PIL import Image\n","import numpy as np\n","import sklearn\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h6lL8z8HNnMP","outputId":"d2f50802-0ad2-4e29-f5f8-8dea6a783866","executionInfo":{"status":"ok","timestamp":1589366138593,"user_tz":-540,"elapsed":5181,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["temp = Image.open(imgs[0])\n","temp2 = torch.LongTensor(np.array(temp))\n","temp2.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 64, 3])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"PD9YtTObWeIB","colab_type":"code","colab":{}},"source":["# y_train = np.array([0.]*26 + [1.]*26)\n","y_train = np.array([0.]*90)\n","# y_train = np.array([0.]*90 + [1.]*52)\n","y_train = torch.LongTensor(y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTLfAOZhVAXd","colab_type":"code","outputId":"e1a2b7af-68e9-46da-f2b0-a7fdf879272e","executionInfo":{"status":"ok","timestamp":1589368223065,"user_tz":-540,"elapsed":1687,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["img_arr = list()\n","# scaler = StandardScaler()\n","for img in imgs:\n","  temp = Image.open(img)\n","  temp = temp.convert(\"L\")\n","  temp2 = np.array(temp)\n","  # temp2 =  scaler.fit_transform(np.array(temp))\n","  img_arr.append(temp2)\n","  # print(temp2)\n","  # break\n","len(img_arr)"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["90"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"bZGqMeL3W_F7","colab_type":"code","colab":{}},"source":["a = torch.Tensor(np.array(img_arr)).cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGRmiJlHcQgF","colab_type":"code","colab":{}},"source":["q1 = a.unsqueeze(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Pnvgk03cQcF","colab_type":"code","outputId":"d65c0914-ad55-4795-bbe6-4d6ae9cd8719","executionInfo":{"status":"ok","timestamp":1589368226991,"user_tz":-540,"elapsed":1023,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["q1.size()"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([90, 1, 64, 64])"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"X7sc6WLUVAnU","colab_type":"code","colab":{}},"source":["from torch.utils.data import TensorDataset \n","from torch.utils.data import DataLoader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NF-zXxzW_Tw","colab_type":"code","colab":{}},"source":["dataset = TensorDataset(q1, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQOd3TfhgYzG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j06ZCdr_dWVh","colab_type":"text"},"source":["# 코드 정리하기!!!\n","\n","<hr>\n"]},{"cell_type":"code","metadata":{"id":"p-FW9lxxnd3D","colab_type":"code","colab":{}},"source":["#데이터를 한번에 batch_size만큼만 가져오는 dataloader를 만든다.\n","# dataloader =DataLoader(dataset, batch_size=10, shuffle=True)\n","dataloader =DataLoader(dataset, batch_size=30, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-ycdv4uw28Z","colab_type":"text"},"source":["### 2. 모델 구축(생성자 & 구분자)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sfr-U67d9qMH","colab":{}},"source":["# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n","class Generator(nn.Module):\n","\n","  # 네트워크 구조\n","    def __init__(self):\n","      super(Generator, self).__init__()\n","      self.main = nn.Sequential(\n","        nn.Linear(in_features=100, out_features=256),\n","        nn.LeakyReLU(0.2),\n","        # nn.Dropout(0.1),\n","        nn.Linear(in_features=256, out_features=512),\n","        nn.LeakyReLU(0.2),\n","        # nn.Dropout(0.1),\n","        nn.Linear(in_features=512, out_features=1024),\n","        nn.LeakyReLU(0.2),\n","        # nn.Dropout(0.1),\n","        nn.Linear(in_features=1024, out_features=64*64),\n","        nn.Tanh())\n","    \n","  # (batch_size x 100) 크기의 랜덤 벡터를 받아 \n","  # 이미지를 (batch_size x 1 x 28 x 28) 크기로 출력한다.\n","    def forward(self, inputs):\n","      return self.main(inputs).view(-1, 1, 64, 64)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"05X_WGjwvv8y"},"source":["#### 구분자(Discriminator) 구축\n","  - 이미지를 입력으로 받고 그 이미지가 진짜일 확률을 0과 1 사이의 숫자 하나로 출력하는 함수"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a70aRCRav8Pn"},"source":["#### 사용하는 모델\n","- 4개의 선형 레이어\n","  - 이미지(28x28) => 레이어(1024 뉴런)  \n","    => 레이어(512) => 레이어(256) => 마지막 예측 레이어(1)\n","- 활성 함수로는 LeakyReLU ( 0보다 낮은 값은 0.2(설정값)을 곱 )\n","  - 마지막 레이어 : Sigmoid ( 출략값이 0~1 이기 때문에 )\n","- 드롭아웃 층\n","  - 과적합(Overfitting, 오버피팅)되는 것을 방지\n","  - 구분자가 생성자보다 지나치게 빨리 학습되는 것 막기 위함."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wu2cTq3P9ivw","colab":{}},"source":["# 구분자는 이미지를 입력으로 받아 이미지가 진짜인지 가짜인지 출력한다.\n","class Discriminator(nn.Module):\n","    \n","# 네트워크 구조\n","  def __init__(self):\n","    super(Discriminator, self).__init__()\n","    self.main = nn.Sequential(\n","      nn.Linear(in_features=64*64, out_features=1024),\n","      nn.LeakyReLU(0.1, ),\n","      # nn.Dropout(0.1),\n","      nn.Linear(in_features=1024, out_features=512),\n","      nn.LeakyReLU(0.1, ),\n","      # nn.Dropout(0.1),\n","      nn.Linear(in_features=512, out_features=256),\n","      nn.LeakyReLU(0.1, ),\n","      # nn.Dropout(0.1),\n","      nn.Linear(in_features=256, out_features=1),\n","      nn.Sigmoid())\n","    \n","  # (batch_size x 1 x 28 x 28) 크기의 이미지를 받아\n","  # 이미지가 진짜일 확률을 0~1 사이로 출력한다.\n","  def forward(self, inputs):\n","    inputs = inputs.view(-1, 64*64)\n","    return self.main(inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eIM3DZ3hwx3T","colab_type":"text"},"source":["#### 생성자 & 구분자 객체 생성"]},{"cell_type":"code","metadata":{"id":"2d8iOT10neF0","colab_type":"code","outputId":"e740cfd7-2b4d-4e08-cd6c-4eec9bbeffb1","executionInfo":{"status":"ok","timestamp":1589368242311,"user_tz":-540,"elapsed":1617,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["G = Generator()\n","D = Discriminator()\n","\n","if use_gpu:\n","    print(\"using_GPU\")\n","    G.cuda()\n","    D.cuda()"],"execution_count":64,"outputs":[{"output_type":"stream","text":["using_GPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fMG3fmwRneIE","colab_type":"text"},"source":["### 손실함수 & 최적화기법 지정"]},{"cell_type":"markdown","metadata":{"id":"L2q7MMFLneKF","colab_type":"text"},"source":["- 손실 함수\n","  - Binary Cross Entropy Loss function\n","    - 확률이 정답에 가까워지면 낮아지고, 멀면 높아진다.\n","    - 낮추는 것이 목표\n","- 최적화 기법 \n","  - Adam\n","    - 매개변수마다 업데이트 속도를 최적으로 조절하는 효육적인 최적화 기법"]},{"cell_type":"code","metadata":{"id":"cg0cnKjgneMU","colab_type":"code","colab":{}},"source":["# Binary Cross Entropy loss\n","# criterion = nn.BCELoss()\n","# criterion = nn.BCEWithLogitsLoss()\n","criterion = nn.BCELoss()\n","\n","# 생성자의 매개 변수를 최적화하는 Adam optimizer\n","G_optimizer = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","# 구분자의 매개 변수를 최적화하는 Adam optimizer\n","D_optimizer = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9AJKZgBq0Vfe","colab_type":"text"},"source":["### 시각화 함수"]},{"cell_type":"code","metadata":{"id":"Zu7XcfXo0V27","colab_type":"code","colab":{}},"source":["# 학습 결과 시각화하기\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","def square_plot(data, path):\n","    \"\"\"Take an array of shape (n, height, width) or (n, height, width , 3)\n","       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n","\n","    if type(data) == list:\n","\t    data = np.concatenate(data)\n","    # normalize data for display\n","    data = (data - data.min()) / (data.max() - data.min())\n","\n","    # force the number of filters to be square\n","    n = int(np.ceil(np.sqrt(data.shape[0])))\n","\n","    padding = (((0, n ** 2 - data.shape[0]) ,\n","                (0, 1), (0, 1))  # add some space between filters\n","               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n","    data = np.pad(data , padding, mode='constant' , constant_values=1)  # pad with ones (white)\n","\n","    # tilethe filters into an image\n","    data = data.reshape((n , n) + data.shape[1:]).transpose((0 , 2 , 1 , 3) + tuple(range(4 , data.ndim + 1)))\n","\n","    data = data.reshape((n * data.shape[1] , n * data.shape[3]) + data.shape[4:])\n","\n","    plt.imsave(path, data, cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kQMfpwd0cbQ","colab_type":"code","outputId":"a254b12c-57b5-40ed-969f-cadab59cff95","executionInfo":{"status":"ok","timestamp":1589368163866,"user_tz":-540,"elapsed":1746,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["if leave_log:\n","    train_hist = {}\n","    train_hist['D_losses'] = []\n","    train_hist['G_losses'] = []\n","    generated_images = []\n","    \n","z_fixed = Variable(torch.randn(5 * 5, 100), volatile=True)\n","if use_gpu:\n","    z_fixed = z_fixed.cuda()"],"execution_count":52,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  import sys\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Hsil_3u1neOJ","colab_type":"text"},"source":["### 모델 반복학습"]},{"cell_type":"markdown","metadata":{"id":"vtMVmtJpneQk","colab_type":"text"},"source":["- 100 epoch  \n","  => 데이터셋을 100번 순회\n","- 60 batch size  \n","  => epoch마다 60개의 데이터를 가져오겠다.\n","\n","MNIST 학습 데이터의 개수가 6만개이니 1에폭마다 1000번씩 학습이 이루어지는 셈이다."]},{"cell_type":"code","metadata":{"id":"zHQiuAKHneSo","colab_type":"code","outputId":"534136b8-77b8-4506-d94f-0da0a01cf5af","executionInfo":{"status":"error","timestamp":1589368258197,"user_tz":-540,"elapsed":11887,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from time import time\n","\n","start = time()\n","# 데이터셋을 100번 돌며 학습한다.\n","for epoch in range(100):\n","    \n","    if leave_log:\n","        D_losses = []\n","        G_losses = []\n","    \n","    # 한번에 batch_size만큼 데이터를 가져온다.\n","    for real_data, _ in dataloader:\n","        batch_size = real_data.size(0)\n","        # print(batch_size)\n","        \n","        # 데이터를 pytorch의 변수로 변환한다.\n","        real_data = Variable(real_data)\n","        # print(real_data.size())\n","\n","        ### 구분자 학습시키기\n","\n","        # 이미지가 진짜일 때 정답 값은 1이고 가짜일 때는 0이다.\n","        # 정답지에 해당하는 변수를 만든다.\n","        target_real = Variable(torch.ones(batch_size, 1))\n","        target_fake = Variable(torch.zeros(batch_size, 1))\n","         \n","        if use_gpu:\n","            real_data, target_real, target_fake = real_data.cuda(), target_real.cuda(), target_fake.cuda()\n","            \n","        # 진짜 이미지를 구분자에 넣는다.\n","        D_result_from_real = D(real_data)\n","        # 구분자의 출력값이 정답지인 1에서 멀수록 loss가 높아진다.\n","        D_loss_real = criterion(D_result_from_real, target_real)\n","\n","        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n","        z = Variable(torch.randn((batch_size, 100)))\n","        \n","        if use_gpu:\n","            z = z.cuda()\n","            \n","        # 생성자로 가짜 이미지를 생성한다.\n","        fake_data = G(z)\n","        \n","        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n","        D_result_from_fake = D(fake_data)\n","        # 구분자의 출력값이 정답지인 0에서 멀수록 loss가 높아진다.\n","        D_loss_fake = criterion(D_result_from_fake, target_fake)\n","        \n","        # 구분자의 loss는 두 문제에서 계산된 loss의 합이다.\n","        D_loss = D_loss_real + D_loss_fake\n","        \n","        # 구분자의 매개 변수의 미분값을 0으로 초기화한다.\n","        D.zero_grad()\n","        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n","        D_loss.backward()\n","        # 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\n","        D_optimizer.step()\n","        \n","        if leave_log:\n","            # D_losses.append(D_loss.data[0])\n","            D_losses.append(D_loss.item())\n","\n","        # train generator G\n","\n","        ### 생성자 학습시키기\n","        \n","        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n","        z = Variable(torch.randn((batch_size, 100)))\n","        \n","        if use_gpu:\n","            z = z.cuda()\n","        \n","        # 생성자로 가짜 이미지를 생성한다.\n","        fake_data = G(z)\n","        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n","        D_result_from_fake = D(fake_data)\n","        # 생성자의 입장에서 구분자의 출력값이 1에서 멀수록 loss가 높아진다.\n","        G_loss = criterion(D_result_from_fake, target_real)\n","        \n","        # 생성자의 매개 변수의 미분값을 0으로 초기화한다.\n","        G.zero_grad()\n","        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n","        G_loss.backward()\n","        # 최적화 기법을 이용해 생성자의 매개 변수를 업데이트한다.\n","        G_optimizer.step()\n","        \n","        if leave_log:\n","            # G_losses.append(G_loss.data[0])\n","            G_losses.append(G_loss.item())\n","    if leave_log:\n","        # true_positive_rate = (D_result_from_real > 0.5).float().mean().data[0]\n","        # true_negative_rate = (D_result_from_fake < 0.5).float().mean().data[0]\n","        true_positive_rate = (D_result_from_real > 0.5).float().mean().item()\n","        true_negative_rate = (D_result_from_fake < 0.5).float().mean().item()\n","        base_message = (\"Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} \"\n","                        \"True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}\"\n","                       )\n","        message = base_message.format(\n","                    epoch=epoch,\n","                    d_loss=sum(D_losses)/len(D_losses),\n","                    g_loss=sum(G_losses)/len(G_losses),\n","                    tpr=true_positive_rate,\n","                    tnr=true_negative_rate\n","        )\n","        print(message)\n","    \n","    if leave_log:\n","        fake_data_fixed = G(z_fixed)\n","        image_path = result_dir + '/epoch{}.png'.format(epoch)\n","        square_plot(fake_data_fixed.view(25, 64, 64).cpu().data.numpy(), path=image_path)\n","        generated_images.append(image_path)\n","    \n","    if leave_log:\n","        train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n","        train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n","\n","torch.save(G.state_dict(), \"gan_generator.pkl\")\n","torch.save(D.state_dict(), \"gan_discriminator.pkl\")\n","with open('gan_train_history.pkl', 'wb') as f:\n","    pickle.dump(train_hist, f)\n","\n","generated_image_array = [imageio.imread(generated_image) for generated_image in generated_images]\n","imageio.mimsave(result_dir + '/GAN_generation.gif', generated_image_array, fps=5)\n","\n","print(\"end_time : \", str(start-time()))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["Epoch: 0   D Loss: 3.35653  G Loss: 0.707976 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 1   D Loss: 3.3565   G Loss: 0.707965 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 2   D Loss: 3.35655  G Loss: 0.707918 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 3   D Loss: 3.35651  G Loss: 0.707994 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 4   D Loss: 3.35664  G Loss: 0.707891 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 5   D Loss: 3.35669  G Loss: 0.707942 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 6   D Loss: 3.3565   G Loss: 0.707926 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 7   D Loss: 3.35657  G Loss: 0.707957 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 8   D Loss: 3.35655  G Loss: 0.708028 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 9   D Loss: 3.3565   G Loss: 0.707993 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 10  D Loss: 3.3566   G Loss: 0.707875 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 11  D Loss: 3.35653  G Loss: 0.70798  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 12  D Loss: 3.35659  G Loss: 0.707979 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 13  D Loss: 3.35664  G Loss: 0.708024 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 14  D Loss: 3.35655  G Loss: 0.707971 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 15  D Loss: 3.3565   G Loss: 0.707991 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 16  D Loss: 3.35664  G Loss: 0.707934 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 17  D Loss: 3.3565   G Loss: 0.708007 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 18  D Loss: 3.35657  G Loss: 0.708056 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 19  D Loss: 3.35671  G Loss: 0.707967 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 20  D Loss: 3.35658  G Loss: 0.707936 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 21  D Loss: 3.35645  G Loss: 0.707966 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 22  D Loss: 3.3565   G Loss: 0.707837 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 23  D Loss: 3.35652  G Loss: 0.707985 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 24  D Loss: 3.35654  G Loss: 0.70795  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 25  D Loss: 3.35657  G Loss: 0.70796  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 26  D Loss: 3.35658  G Loss: 0.70791  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 27  D Loss: 3.35653  G Loss: 0.707915 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 28  D Loss: 3.35643  G Loss: 0.707905 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 29  D Loss: 3.35652  G Loss: 0.707974 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 30  D Loss: 3.35649  G Loss: 0.70794  True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 31  D Loss: 3.35648  G Loss: 0.707965 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 32  D Loss: 3.35651  G Loss: 0.707931 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 33  D Loss: 3.35666  G Loss: 0.707899 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 34  D Loss: 3.35655  G Loss: 0.707974 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 35  D Loss: 3.35649  G Loss: 0.707935 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 36  D Loss: 3.35655  G Loss: 0.708003 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 37  D Loss: 3.3565   G Loss: 0.707798 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 38  D Loss: 3.35651  G Loss: 0.707896 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 39  D Loss: 3.35652  G Loss: 0.708062 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 40  D Loss: 3.35649  G Loss: 0.707873 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 41  D Loss: 3.35659  G Loss: 0.708039 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 42  D Loss: 3.3565   G Loss: 0.707943 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 43  D Loss: 3.35648  G Loss: 0.707941 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 44  D Loss: 3.35661  G Loss: 0.708038 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 45  D Loss: 3.35657  G Loss: 0.708029 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 46  D Loss: 3.3565   G Loss: 0.707958 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 47  D Loss: 3.35653  G Loss: 0.707976 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 48  D Loss: 3.35646  G Loss: 0.707929 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 49  D Loss: 3.35653  G Loss: 0.707923 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 50  D Loss: 3.35662  G Loss: 0.707996 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 51  D Loss: 3.35654  G Loss: 0.707931 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 52  D Loss: 3.35656  G Loss: 0.707967 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 53  D Loss: 3.35654  G Loss: 0.707913 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 54  D Loss: 3.35656  G Loss: 0.707878 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 55  D Loss: 3.35653  G Loss: 0.70791  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 56  D Loss: 3.35645  G Loss: 0.708004 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 57  D Loss: 3.35654  G Loss: 0.707969 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 58  D Loss: 3.35651  G Loss: 0.707826 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 59  D Loss: 3.35656  G Loss: 0.707908 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 60  D Loss: 3.35661  G Loss: 0.708019 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 61  D Loss: 3.35656  G Loss: 0.708019 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 62  D Loss: 3.35654  G Loss: 0.708047 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 63  D Loss: 3.35657  G Loss: 0.708017 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 64  D Loss: 3.35648  G Loss: 0.707895 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 65  D Loss: 3.35639  G Loss: 0.708025 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 66  D Loss: 3.35652  G Loss: 0.70801  True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 67  D Loss: 3.35651  G Loss: 0.707948 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 68  D Loss: 3.35656  G Loss: 0.707978 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 69  D Loss: 3.35654  G Loss: 0.707948 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 70  D Loss: 3.3565   G Loss: 0.707821 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 71  D Loss: 3.35654  G Loss: 0.707927 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 72  D Loss: 3.35646  G Loss: 0.707916 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 73  D Loss: 3.35656  G Loss: 0.708041 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 74  D Loss: 3.3564   G Loss: 0.707926 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 75  D Loss: 3.35661  G Loss: 0.70796  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 76  D Loss: 3.35656  G Loss: 0.707872 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 77  D Loss: 3.35662  G Loss: 0.70799  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 78  D Loss: 3.35657  G Loss: 0.707842 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 79  D Loss: 3.35649  G Loss: 0.707861 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 80  D Loss: 3.3565   G Loss: 0.708078 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 81  D Loss: 3.3566   G Loss: 0.707903 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 82  D Loss: 3.35653  G Loss: 0.707964 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 83  D Loss: 3.3564   G Loss: 0.708003 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 84  D Loss: 3.35654  G Loss: 0.708037 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 85  D Loss: 3.35657  G Loss: 0.707865 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 86  D Loss: 3.35655  G Loss: 0.708046 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 87  D Loss: 3.35652  G Loss: 0.707972 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 88  D Loss: 3.35656  G Loss: 0.708033 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 89  D Loss: 3.35656  G Loss: 0.708016 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 90  D Loss: 3.35659  G Loss: 0.707979 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 91  D Loss: 3.35644  G Loss: 0.707972 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 92  D Loss: 3.35645  G Loss: 0.70797  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 93  D Loss: 3.35654  G Loss: 0.707987 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 94  D Loss: 3.35653  G Loss: 0.707892 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 95  D Loss: 3.35647  G Loss: 0.707946 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 96  D Loss: 3.35656  G Loss: 0.707981 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 97  D Loss: 3.35656  G Loss: 0.707864 True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 98  D Loss: 3.35647  G Loss: 0.707902 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n","Epoch: 99  D Loss: 3.35646  G Loss: 0.707983 True Positive Rate: 3.3%  True Negative Rate: 100.0%\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-226db604b9c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mgenerated_image_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/GAN_generation.gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_image_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-226db604b9c8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mgenerated_image_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/GAN_generation.gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_image_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename_zip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename_zip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEXAMPLE_IMAGES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 raise IOError(\n\u001b[1;32m    249\u001b[0m                     \u001b[0;34m\"No such file: %r. This file looks like one of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"w9qeGSgpuXpe","colab_type":"code","outputId":"b2f6b930-cb88-42d9-a54f-ffac28cf426d","executionInfo":{"status":"ok","timestamp":1589265126442,"user_tz":-540,"elapsed":601,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["z = Variable(torch.randn((batch_size, 80)))\n","z"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.1534,  1.4060,  1.0578,  ..., -2.3254,  0.3431,  1.3877],\n","        [ 1.3626,  1.8769,  1.2459,  ...,  0.0521,  1.2031, -1.5178],\n","        [ 0.0888,  0.9181, -0.9085,  ..., -1.6978,  1.5547, -2.3547],\n","        ...,\n","        [ 0.3066,  0.8023,  0.2619,  ...,  1.0835, -0.1802, -0.5282],\n","        [-0.7603,  0.1801,  1.6961,  ..., -0.0572,  1.0028,  0.8055],\n","        [ 0.8609,  0.0818,  0.4082,  ..., -1.5827, -1.1080, -0.7029]])"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"Iab9hmXdkELx","colab_type":"code","outputId":"82d8e62e-ad38-4c5c-cdf7-4669ae3e6c99","executionInfo":{"status":"ok","timestamp":1589265030114,"user_tz":-540,"elapsed":637,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import random\n","random.random()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7635124661404858"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"WTA-Hokhk9j8","colab_type":"text"},"source":["# rand int(0~1)\n","\n","- scaler => 정규분포 교체\n","- vector => randn 교체\n","\n","~~~\n","Epoch: 0   D Loss: 0.990409 G Loss: 1.38524  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 1   D Loss: 1.27203  G Loss: 1.60394  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 2   D Loss: 1.58664  G Loss: 1.18453  True Positive Rate: 30.0% True Negative Rate: 93.3%\n","Epoch: 3   D Loss: 1.12798  G Loss: 1.36241  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 4   D Loss: 1.65854  G Loss: 1.83637  True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 5   D Loss: 1.25826  G Loss: 1.66822  True Positive Rate: 100.0% True Negative Rate: 100.0%\n","Epoch: 6   D Loss: 1.60527  G Loss: 1.40425  True Positive Rate: 53.3% True Negative Rate: 73.3%\n","Epoch: 7   D Loss: 1.87918  G Loss: 0.761097 True Positive Rate: 56.7% True Negative Rate: 76.7%\n","Epoch: 8   D Loss: 1.22751  G Loss: 1.24992  True Positive Rate: 43.3% True Negative Rate: 100.0%\n","Epoch: 9   D Loss: 1.20484  G Loss: 1.26276  True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 10  D Loss: 1.35906  G Loss: 1.27616  True Positive Rate: 23.3% True Negative Rate: 100.0%\n","Epoch: 11  D Loss: 1.43436  G Loss: 1.66363  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 12  D Loss: 2.06688  G Loss: 1.77353  True Positive Rate: 0.0%  True Negative Rate: 100.0%\n","Epoch: 13  D Loss: 1.82243  G Loss: 2.01021  True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 14  D Loss: 0.942322 G Loss: 2.00392  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 15  D Loss: 1.00432  G Loss: 1.65147  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 16  D Loss: 1.46306  G Loss: 1.13022  True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 17  D Loss: 1.69429  G Loss: 0.822099 True Positive Rate: 43.3% True Negative Rate: 100.0%\n","Epoch: 18  D Loss: 1.59064  G Loss: 0.787313 True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 19  D Loss: 1.48329  G Loss: 0.919296 True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 20  D Loss: 1.31983  G Loss: 1.08266  True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 21  D Loss: 1.14992  G Loss: 1.28387  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 22  D Loss: 1.0942   G Loss: 1.4536   True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 23  D Loss: 1.44834  G Loss: 1.72904  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 24  D Loss: 1.73717  G Loss: 1.51053  True Positive Rate: 23.3% True Negative Rate: 100.0%\n","Epoch: 25  D Loss: 1.80419  G Loss: 1.43129  True Positive Rate: 33.3% True Negative Rate: 100.0%\n","Epoch: 26  D Loss: 1.92125  G Loss: 1.37885  True Positive Rate: 6.7%  True Negative Rate: 100.0%\n","Epoch: 27  D Loss: 1.53847  G Loss: 1.38698  True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 28  D Loss: 0.995595 G Loss: 1.70165  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 29  D Loss: 1.06438  G Loss: 1.55487  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 30  D Loss: 1.36968  G Loss: 1.15213  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 31  D Loss: 1.60984  G Loss: 0.955796 True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 32  D Loss: 1.69537  G Loss: 0.743417 True Positive Rate: 46.7% True Negative Rate: 90.0%\n","Epoch: 33  D Loss: 1.53447  G Loss: 0.765797 True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 34  D Loss: 1.36583  G Loss: 0.947653 True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 35  D Loss: 1.31552  G Loss: 1.07511  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 36  D Loss: 1.38344  G Loss: 1.10122  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 37  D Loss: 1.53153  G Loss: 1.12254  True Positive Rate: 30.0% True Negative Rate: 100.0%\n","Epoch: 38  D Loss: 1.53107  G Loss: 1.15483  True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 39  D Loss: 1.61191  G Loss: 1.38747  True Positive Rate: 20.0% True Negative Rate: 100.0%\n","Epoch: 40  D Loss: 1.61929  G Loss: 1.50426  True Positive Rate: 16.7% True Negative Rate: 100.0%\n","Epoch: 41  D Loss: 1.31622  G Loss: 1.6568   True Positive Rate: 20.0% True Negative Rate: 100.0%\n","Epoch: 42  D Loss: 1.09822  G Loss: 1.78888  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 43  D Loss: 1.25877  G Loss: 1.57229  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 44  D Loss: 1.31139  G Loss: 1.42331  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 45  D Loss: 1.46987  G Loss: 1.16977  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 46  D Loss: 1.47163  G Loss: 0.937221 True Positive Rate: 33.3% True Negative Rate: 100.0%\n","Epoch: 47  D Loss: 1.32611  G Loss: 0.918541 True Positive Rate: 36.7% True Negative Rate: 100.0%\n","Epoch: 48  D Loss: 1.21565  G Loss: 0.947456 True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 49  D Loss: 1.16054  G Loss: 1.03136  True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 50  D Loss: 1.18133  G Loss: 0.961517 True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 51  D Loss: 1.20286  G Loss: 0.968519 True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 52  D Loss: 1.19391  G Loss: 0.899091 True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 53  D Loss: 1.17388  G Loss: 0.931549 True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 54  D Loss: 1.13433  G Loss: 0.958241 True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 55  D Loss: 1.05674  G Loss: 1.05889  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 56  D Loss: 0.935457 G Loss: 1.1455   True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 57  D Loss: 0.980629 G Loss: 1.28014  True Positive Rate: 96.7% True Negative Rate: 100.0%\n","Epoch: 58  D Loss: 1.04478  G Loss: 1.37386  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 59  D Loss: 1.08758  G Loss: 1.35638  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 60  D Loss: 0.906913 G Loss: 1.43278  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 61  D Loss: 0.833566 G Loss: 1.49659  True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 62  D Loss: 0.974353 G Loss: 1.24301  True Positive Rate: 86.7% True Negative Rate: 100.0%\n","Epoch: 63  D Loss: 1.03351  G Loss: 1.07428  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 64  D Loss: 1.07226  G Loss: 1.0666   True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 65  D Loss: 1.10038  G Loss: 1.20801  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 66  D Loss: 1.05886  G Loss: 1.13145  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 67  D Loss: 0.953486 G Loss: 1.16276  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 68  D Loss: 0.945249 G Loss: 1.3091   True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 69  D Loss: 1.07632  G Loss: 1.45596  True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 70  D Loss: 1.22053  G Loss: 1.49016  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 71  D Loss: 1.42419  G Loss: 1.58302  True Positive Rate: 13.3% True Negative Rate: 100.0%\n","Epoch: 72  D Loss: 1.30198  G Loss: 1.30961  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 73  D Loss: 1.03926  G Loss: 1.48842  True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 74  D Loss: 0.910021 G Loss: 1.40388  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 75  D Loss: 0.879891 G Loss: 1.30806  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 76  D Loss: 1.02487  G Loss: 1.14405  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 77  D Loss: 1.19096  G Loss: 0.968821 True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 78  D Loss: 1.24043  G Loss: 0.851859 True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 79  D Loss: 1.36529  G Loss: 0.86162  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 80  D Loss: 1.47393  G Loss: 0.77695  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 81  D Loss: 1.48082  G Loss: 0.768555 True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 82  D Loss: 1.39429  G Loss: 0.964111 True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 83  D Loss: 1.26467  G Loss: 1.16427  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 84  D Loss: 1.27106  G Loss: 1.33496  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 85  D Loss: 1.57101  G Loss: 1.55991  True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 86  D Loss: 1.76473  G Loss: 1.57733  True Positive Rate: 23.3% True Negative Rate: 100.0%\n","Epoch: 87  D Loss: 1.65184  G Loss: 1.76637  True Positive Rate: 20.0% True Negative Rate: 100.0%\n","Epoch: 88  D Loss: 1.44276  G Loss: 1.88189  True Positive Rate: 33.3% True Negative Rate: 100.0%\n","Epoch: 89  D Loss: 1.35472  G Loss: 2.11164  True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 90  D Loss: 1.47351  G Loss: 1.9533   True Positive Rate: 36.7% True Negative Rate: 100.0%\n","Epoch: 91  D Loss: 1.67061  G Loss: 1.41472  True Positive Rate: 10.0% True Negative Rate: 100.0%\n","Epoch: 92  D Loss: 1.47795  G Loss: 1.16351  True Positive Rate: 30.0% True Negative Rate: 100.0%\n","Epoch: 93  D Loss: 1.0964   G Loss: 1.36057  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 94  D Loss: 0.944807 G Loss: 1.52948  True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 95  D Loss: 1.09354  G Loss: 1.10983  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 96  D Loss: 1.26797  G Loss: 0.978764 True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 97  D Loss: 1.22352  G Loss: 1.04862  True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 98  D Loss: 1.11339  G Loss: 1.1801   True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 99  D Loss: 1.10983  G Loss: 1.20909  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 100 D Loss: 1.05326  G Loss: 1.30998  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 101 D Loss: 1.19563  G Loss: 1.4147   True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 102 D Loss: 1.24383  G Loss: 1.25892  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 103 D Loss: 1.01744  G Loss: 1.35167  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 104 D Loss: 1.10814  G Loss: 1.46978  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 105 D Loss: 1.48636  G Loss: 1.12243  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 106 D Loss: 1.35668  G Loss: 0.87474  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 107 D Loss: 1.12039  G Loss: 1.01572  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 108 D Loss: 0.999875 G Loss: 1.1419   True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 109 D Loss: 1.01567  G Loss: 1.22925  True Positive Rate: 43.3% True Negative Rate: 100.0%\n","Epoch: 110 D Loss: 1.09361  G Loss: 1.17642  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 111 D Loss: 1.20503  G Loss: 1.10615  True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 112 D Loss: 1.26107  G Loss: 1.08401  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 113 D Loss: 1.24478  G Loss: 1.04437  True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 114 D Loss: 1.12806  G Loss: 1.15327  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 115 D Loss: 0.99158  G Loss: 1.33976  True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 116 D Loss: 0.936419 G Loss: 1.33594  True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 117 D Loss: 0.991113 G Loss: 1.58834  True Positive Rate: 100.0% True Negative Rate: 100.0%\n","Epoch: 118 D Loss: 1.00892  G Loss: 1.38297  True Positive Rate: 96.7% True Negative Rate: 100.0%\n","Epoch: 119 D Loss: 1.04912  G Loss: 1.27545  True Positive Rate: 86.7% True Negative Rate: 100.0%\n","Epoch: 120 D Loss: 1.04064  G Loss: 1.26572  True Positive Rate: 90.0% True Negative Rate: 100.0%\n","Epoch: 121 D Loss: 0.830799 G Loss: 1.21753  True Positive Rate: 100.0% True Negative Rate: 100.0%\n","Epoch: 122 D Loss: 0.631105 G Loss: 1.35053  True Positive Rate: 93.3% True Negative Rate: 100.0%\n","Epoch: 123 D Loss: 0.713063 G Loss: 1.38922  True Positive Rate: 86.7% True Negative Rate: 100.0%\n","Epoch: 124 D Loss: 0.728369 G Loss: 1.3948   True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 125 D Loss: 0.89531  G Loss: 1.33786  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 126 D Loss: 1.49208  G Loss: 1.09694  True Positive Rate: 36.7% True Negative Rate: 100.0%\n","Epoch: 127 D Loss: 1.55071  G Loss: 1.13239  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 128 D Loss: 1.51611  G Loss: 1.03071  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 129 D Loss: 1.43756  G Loss: 0.95797  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 130 D Loss: 1.22826  G Loss: 1.06183  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 131 D Loss: 0.979778 G Loss: 1.25856  True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 132 D Loss: 0.83276  G Loss: 1.50409  True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 133 D Loss: 0.918514 G Loss: 1.62083  True Positive Rate: 86.7% True Negative Rate: 100.0%\n","Epoch: 134 D Loss: 1.38293  G Loss: 1.49804  True Positive Rate: 13.3% True Negative Rate: 100.0%\n","Epoch: 135 D Loss: 1.42486  G Loss: 1.13696  True Positive Rate: 30.0% True Negative Rate: 100.0%\n","Epoch: 136 D Loss: 1.03438  G Loss: 1.07715  True Positive Rate: 100.0% True Negative Rate: 100.0%\n","Epoch: 137 D Loss: 0.945063 G Loss: 1.26919  True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 138 D Loss: 1.1709   G Loss: 1.21613  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 139 D Loss: 1.21785  G Loss: 1.04643  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 140 D Loss: 1.17296  G Loss: 1.18513  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 141 D Loss: 1.16436  G Loss: 1.24095  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 142 D Loss: 1.18972  G Loss: 1.32905  True Positive Rate: 30.0% True Negative Rate: 100.0%\n","Epoch: 143 D Loss: 1.18876  G Loss: 1.19391  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 144 D Loss: 1.14434  G Loss: 1.11968  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 145 D Loss: 1.04499  G Loss: 1.13418  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 146 D Loss: 0.925552 G Loss: 1.31303  True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 147 D Loss: 0.86291  G Loss: 1.36935  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 148 D Loss: 0.813161 G Loss: 1.37835  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 149 D Loss: 0.864099 G Loss: 1.43637  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 150 D Loss: 1.43143  G Loss: 1.41343  True Positive Rate: 30.0% True Negative Rate: 100.0%\n","Epoch: 151 D Loss: 1.92235  G Loss: 0.892886 True Positive Rate: 23.3% True Negative Rate: 83.3%\n","Epoch: 152 D Loss: 1.98533  G Loss: 0.626468 True Positive Rate: 20.0% True Negative Rate: 3.3% \n","Epoch: 153 D Loss: 1.75206  G Loss: 0.735035 True Positive Rate: 33.3% True Negative Rate: 100.0%\n","Epoch: 154 D Loss: 1.48477  G Loss: 0.932511 True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 155 D Loss: 1.3253   G Loss: 1.0705   True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 156 D Loss: 1.2677   G Loss: 1.15755  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 157 D Loss: 1.32031  G Loss: 1.16855  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 158 D Loss: 1.39323  G Loss: 1.18267  True Positive Rate: 36.7% True Negative Rate: 100.0%\n","Epoch: 159 D Loss: 1.46966  G Loss: 1.09076  True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 160 D Loss: 1.43362  G Loss: 1.09079  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 161 D Loss: 1.38274  G Loss: 1.06737  True Positive Rate: 40.0% True Negative Rate: 100.0%\n","Epoch: 162 D Loss: 1.32184  G Loss: 1.10149  True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 163 D Loss: 1.15025  G Loss: 1.19762  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 164 D Loss: 0.992689 G Loss: 1.27865  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 165 D Loss: 0.978424 G Loss: 1.2633   True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 166 D Loss: 1.03536  G Loss: 1.12888  True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 167 D Loss: 1.40182  G Loss: 1.01998  True Positive Rate: 50.0% True Negative Rate: 100.0%\n","Epoch: 168 D Loss: 1.72167  G Loss: 0.831796 True Positive Rate: 26.7% True Negative Rate: 100.0%\n","Epoch: 169 D Loss: 1.44017  G Loss: 0.734123 True Positive Rate: 80.0% True Negative Rate: 66.7%\n","Epoch: 170 D Loss: 1.04735  G Loss: 1.03696  True Positive Rate: 80.0% True Negative Rate: 100.0%\n","Epoch: 171 D Loss: 0.854695 G Loss: 1.30731  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 172 D Loss: 0.921275 G Loss: 1.43761  True Positive Rate: 86.7% True Negative Rate: 100.0%\n","Epoch: 173 D Loss: 1.09996  G Loss: 1.21177  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 174 D Loss: 1.11126  G Loss: 1.11736  True Positive Rate: 46.7% True Negative Rate: 100.0%\n","Epoch: 175 D Loss: 1.11037  G Loss: 1.04184  True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 176 D Loss: 1.06262  G Loss: 1.07149  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 177 D Loss: 1.09627  G Loss: 1.06586  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 178 D Loss: 1.04001  G Loss: 1.19953  True Positive Rate: 70.0% True Negative Rate: 100.0%\n","Epoch: 179 D Loss: 1.0151   G Loss: 1.14341  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 180 D Loss: 0.922163 G Loss: 1.25886  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 181 D Loss: 0.884992 G Loss: 1.26936  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 182 D Loss: 0.768008 G Loss: 1.30016  True Positive Rate: 76.7% True Negative Rate: 100.0%\n","Epoch: 183 D Loss: 0.682484 G Loss: 1.28988  True Positive Rate: 93.3% True Negative Rate: 100.0%\n","Epoch: 184 D Loss: 0.635521 G Loss: 1.34204  True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 185 D Loss: 0.647937 G Loss: 1.3585   True Positive Rate: 86.7% True Negative Rate: 100.0%\n","Epoch: 186 D Loss: 0.665769 G Loss: 1.31946  True Positive Rate: 93.3% True Negative Rate: 100.0%\n","Epoch: 187 D Loss: 0.688987 G Loss: 1.30906  True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 188 D Loss: 0.803481 G Loss: 1.14138  True Positive Rate: 90.0% True Negative Rate: 100.0%\n","Epoch: 189 D Loss: 0.950419 G Loss: 1.00811  True Positive Rate: 73.3% True Negative Rate: 100.0%\n","Epoch: 190 D Loss: 1.39841  G Loss: 0.716276 True Positive Rate: 60.0% True Negative Rate: 36.7%\n","Epoch: 191 D Loss: 1.31047  G Loss: 0.829697 True Positive Rate: 96.7% True Negative Rate: 100.0%\n","Epoch: 192 D Loss: 0.831449 G Loss: 1.60141  True Positive Rate: 100.0% True Negative Rate: 100.0%\n","Epoch: 193 D Loss: 0.973539 G Loss: 1.96865  True Positive Rate: 60.0% True Negative Rate: 100.0%\n","Epoch: 194 D Loss: 1.22514  G Loss: 1.49774  True Positive Rate: 53.3% True Negative Rate: 100.0%\n","Epoch: 195 D Loss: 0.947257 G Loss: 1.02785  True Positive Rate: 83.3% True Negative Rate: 100.0%\n","Epoch: 196 D Loss: 0.924251 G Loss: 1.32346  True Positive Rate: 66.7% True Negative Rate: 100.0%\n","Epoch: 197 D Loss: 0.927951 G Loss: 1.06843  True Positive Rate: 56.7% True Negative Rate: 100.0%\n","Epoch: 198 D Loss: 0.936711 G Loss: 1.21217  True Positive Rate: 63.3% True Negative Rate: 100.0%\n","Epoch: 199 D Loss: 0.937629 G Loss: 1.2542   True Positive Rate: 60.0% True Negative Rate: 100.0%\n","/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n","end_time :  -34.345648765563965\n","~~~"]},{"cell_type":"code","metadata":{"id":"cKpUpyfomwcn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}