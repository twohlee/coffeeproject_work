{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups_folder_path = '/content/drive/My Drive/beans'\n",
    "# categories = [\"broken\", \"insect\", \"normal\"] # 해당 이미지들이 들어가 있는 폴더명 넣어주기 \n",
    "# num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting sklearn\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\nCollecting scikit-learn\n  Downloading scikit_learn-0.23.0-cp37-cp37m-win_amd64.whl (6.8 MB)\nCollecting joblib>=0.11\n  Downloading joblib-0.15.1-py3-none-any.whl (298 kB)\nRequirement already satisfied: scipy>=0.19.1 in c:\\users\\admin\\anaconda3\\envs\\project_opencv\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\nRequirement already satisfied: numpy>=1.13.3 in c:\\users\\admin\\anaconda3\\envs\\project_opencv\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\nCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-2.0.0-py3-none-any.whl (34 kB)\nBuilding wheels for collected packages: sklearn\n  Building wheel for sklearn (setup.py): started\n  Building wheel for sklearn (setup.py): finished with status 'done'\n  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1320 sha256=039a5a6d34665b365cd6d63473fee7a168aa3d9d3535ec647a162543acf0dc02\n  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\46\\ef\\c3\\157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\nSuccessfully built sklearn\nInstalling collected packages: joblib, threadpoolctl, scikit-learn, sklearn\nSuccessfully installed joblib-0.15.1 scikit-learn-0.23.0 sklearn-0.0 threadpoolctl-2.0.0\nNote: you may need to restart the kernel to use updated packages.\n"
    }
   ],
   "source": [
    "#pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import os \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import gridfs \n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_folder_path = './beans'\n",
    "categories = [\"broken\", \"normal\"] # 해당 이미지들이 들어가 있는 폴더명 넣어주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mongodb에서 사진 불러오기 \n",
    "# # category 나눠서 들어갈 수 있도록 하기 \n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__' :\n",
    "    \n",
    "#     # connect to database \n",
    "#     client = MongoClient('127.0.0.1', 27017)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     # read in the image \n",
    "\n",
    "#     for i,d in enumerate(categories):\n",
    "#         files = os.listdir(groups_folder_path + '/'+ d)\n",
    "#         database = client[d]\n",
    "\n",
    "#         # create a new gridfs object\n",
    "#         fs = gridfs.GridFS(database)\n",
    "\n",
    "#         for f in files : \n",
    "#             img = open(groups_folder_path + '/'+ d +'/' + f, 'rb')\n",
    "#             thedata = img.read()\n",
    "#             # store the data in the database. \n",
    "#             # Returns the id of the file in gridFS \n",
    "#             stored = fs.put(thedata, filename = f)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     # retrieve what was just stored \n",
    "#     #outputdata = fs.get(stored).read()\n",
    "\n",
    "#     # create an output file and store the image in the output file \n",
    "#     # outfilename = './OpenCV/data/20200508_test1_sample1.jpg'\n",
    "#     # output = open(outfilename, 'wb')\n",
    "#     # output.write(outputdata)\n",
    "#     # # close the output file \n",
    "#     # output.close()\n",
    "\n",
    "#     # for experimental code restore to known state and close connection \n",
    "\n",
    "#     #fs.delete(stored)\n",
    "#     #client.drop_database('example')\n",
    "#     client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Note: you may need to restart the kernel to use updated packages.\nERROR: Could not find a version that satisfies the requirement StringIO (from versions: none)\nERROR: No matching distribution found for StringIO\n"
    }
   ],
   "source": [
    "# pip install StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2160, 2160)"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "# img = Image.open(\"./normal_data/normal_align_test_image/20200518_test_inadvance1_off.jpeg\")\n",
    "# img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(28, 1)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data = []\n",
    "label = []\n",
    "\n",
    "# groups_folder_path = './normal_data'\n",
    "# categories = [\"normal_align_test_image\", \"normal_processed_data\", \"normal_raw_data\" ] # 해당 이미지들이 들어가 있는 폴더명 넣어주기 \n",
    "\n",
    "if __name__ == '__main__' :  \n",
    "  client = MongoClient('127.0.0.1', 27017)\n",
    "  # read in the image \n",
    "  for i,d in enumerate(categories):\n",
    "    database  = client[d]\n",
    "    files     = os.listdir(groups_folder_path + '/'+ d)\n",
    "    \n",
    "    # create a new gridfs object\n",
    "    fs        = gridfs.GridFS(database)\n",
    "\n",
    "    #print( '->' +  d )\n",
    "    for f in files : \n",
    "      #print( groups_folder_path + '/'+ d +'/' + f )\n",
    "      fp         = open(groups_folder_path + '/'+ d +'/' + f, 'rb')\n",
    "      # bytes로 읽는다\n",
    "      thedata     = fp.read()            \n",
    "      \n",
    "      image = Image.open(io.BytesIO(thedata))\n",
    "      #image.show()\n",
    "      \n",
    "      \n",
    "      \n",
    "      #print( type(thedata), len(thedata) )\n",
    "      # 디비에 저장\n",
    "      stored      = fs.put(thedata, filename = f)      \n",
    "      # 디비에서 다시 읽는다\n",
    "      outputdata  = fs.get(stored).read()\n",
    "      #print( type(outputdata), len(outputdata) )\n",
    "      #print( '-'*10 )\n",
    "      \n",
    "      thedata2 = Image.open(fp, mode='r')\n",
    "      \n",
    "\n",
    "      #thedata2 = Image.frombytes('RGB', (2160,2160), outputdata, 'raw')      \n",
    "      # #thedata = Image.open(StringIO.StringIO(outputdata))\n",
    "      # # 이미지 흑백으로 변경 \n",
    "      img = thedata2.convert('L')\n",
    "      # # 이미지를 28, 28로 일괄 리사이즈 \n",
    "      resize_img = img.resize((28,28))\n",
    "      black_resize_img = np.asarray(resize_img)\n",
    "      # # 가공한 이미지 추가 \n",
    "      data.append(black_resize_img)\n",
    "      # # label : broken, insect, normal\n",
    "      label.append(i)\n",
    "\n",
    "      fp.close()\n",
    "      #break\n",
    "\n",
    "\n",
    "  client.close()\n",
    "\n",
    "pd.DataFrame(data[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype = 'float32')\n",
    "label = np.array(label, dtype = 'int64')\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(data, label, test_size = 0.1)\n",
    "\n",
    "train_X = torch.from_numpy(train_X).float()\n",
    "train_Y = torch.from_numpy(train_Y).long()\n",
    "\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_Y = torch.from_numpy(test_Y).long()\n",
    "\n",
    "#train = TensorDataset(train_X, train_Y)\n",
    "#train_loader = DataLoader(train, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         ...,\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n\n        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         ...,\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n\n        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         ...,\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n\n        ...,\n\n        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         ...,\n         [ 3.,  4.,  5.,  ...,  0.,  0.,  0.],\n         [ 5.,  4., 24.,  ...,  0.,  0.,  0.],\n         [ 6.,  0., 64.,  ...,  0.,  0.,  0.]],\n\n        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         ...,\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n\n        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         ...,\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0, 1, 0,  ..., 0, 1, 1])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 1., 1., 1.],\n         [0., 0., 0.,  ..., 1., 1., 1.],\n         [0., 0., 0.,  ..., 1., 1., 1.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n        0, 0, 0])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3049, 28, 28])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3049])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([339, 28, 28])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([339])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 차원 맞추기\n",
    "train_X = train_X.unsqueeze(1)\n",
    "test_X = test_X.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3049, 1, 28, 28])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([339, 1, 28, 28])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 GPU를 사용 가능하다면 device 값이 cuda가 되고, 아니라면 cpu가 됩니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.utils.data.dataset.TensorDataset"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "dataset = TensorDataset(train_X, train_Y)\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 첫번째층\n",
    "        # ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1 ),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 두번째층\n",
    "        # ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
    "\n",
    "        # 전결합층 한정으로 가중치 초기화\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "총 배치의 수 : 30\n[Epoch:    1] cost = 0.718078971\n[Epoch:    2] cost = 0.469824076\n[Epoch:    3] cost = 0.37024644\n[Epoch:    4] cost = 0.318217337\n[Epoch:    5] cost = 0.27054131\n[Epoch:    6] cost = 0.255614221\n[Epoch:    7] cost = 0.21690461\n[Epoch:    8] cost = 0.256442755\n[Epoch:    9] cost = 0.206047744\n[Epoch:   10] cost = 0.172464445\n[Epoch:   11] cost = 0.152665615\n[Epoch:   12] cost = 0.156579867\n[Epoch:   13] cost = 0.137855798\n[Epoch:   14] cost = 0.127083644\n[Epoch:   15] cost = 0.116798639\n"
    }
   ],
   "source": [
    "# 모델을 정의합니다.\n",
    "\n",
    "# CNN 모델 정의\n",
    "model = CNN().to(device)\n",
    "# 비용 함수와 옵티마이저를 정의합니다.\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 총 배치의 수를 출력해보겠습니다.\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))\n",
    "# 총 배치의 수 : 600\n",
    "# 총 배치의 수는 600입니다. 그런데 배치 크기를 100으로 했으므로 결국 훈련 데이터는 총 60,000개란 의미입니다. 이제 모델을 훈련시켜보겠습니다. (시간이 꽤 오래 걸립니다.)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y는 레이블.\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.utils.data.dataset.TensorDataset"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# test dataset 만들기 \n",
    "testset = TensorDataset(test_X, test_Y)\n",
    "type(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTest set : Average loss :  0.1505, Accuracy: 279/339 (82%)\n\n"
    }
   ],
   "source": [
    "# 테스트 데이터로 모델 테스트 진행 \n",
    "\"\"\"\n",
    "https://wingnim.tistory.com/36\n",
    "\"\"\"\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0 \n",
    "correct = 0\n",
    "\n",
    "for data, target in testloader: \n",
    "    data = data.to(device)\n",
    "    target = target.to(device) \n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # sum up batch loss \n",
    "    test_loss += criterion(output, target).data\n",
    "\n",
    "    # get the index of the max log-probability \n",
    "    pred = output.data.max(1, keepdim = True)[1]\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "test_loss /= len(testloader.dataset)/batch_size\n",
    "\n",
    "print('\\nTest set : Average loss : {: .4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testloader.dataset), 100. * correct / len(testloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://medium.com/@trilliwon/pytorch-%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5-%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2-%E1%84%92%E1%85%A2%E1%84%87%E1%85%A9%E1%84%80%E1%85%B5-4ceab523cb66\n",
    "\"\"\"\n",
    "\n",
    "# # 테스트 데이터로 테스트 진행 \n",
    "\n",
    "# categories = [\"broken\", \"normal\"]\n",
    "\n",
    "# class_correct = list(0. for i in range(2))\n",
    "# class_total = list(0. for i in range(2))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in testloader:\n",
    "#         images, labels = data \n",
    "#         outputs = net(images)\n",
    "#         _, predicted = torch.max(outputs, 1) \n",
    "#         c = (predicted == labels).squeeze()\n",
    "#         for i in range(4):\n",
    "#              label = labels[i]\n",
    "#              class_correct[label] += c[i].item()\n",
    "#              class_total[label] += 1\n",
    "# for i in range(10):\n",
    "#     print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i]/class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라즈베리파이에서 찍은 사진 \n",
    "# 공유 폴더 이용해서 파일 불러오기 \n",
    "# 파일 불러와서 구축된 CNN 모델에 대입하여 예측 결과 확인하기 \n",
    "\n",
    "import os \n",
    "#os.path.join('Z:', '0525.jpg')\n",
    "pre_img = Image.open('Z:\\\\normal_test.jpg')\n",
    "#pre_img = pre_img.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "이 커피콩은  normal 입니다.\n"
    }
   ],
   "source": [
    "# CNN 모델에 이미지 대입하여 예측결과 확인하기 \n",
    "# 참고 사이트 \n",
    "\n",
    "\"\"\"\n",
    "https://github.com/MLlounge/ML-Project/blob/master/Basic%20Project/CNN%20-%20Distinguish%20language%20according%20to%20characters/CNN%20-%20Distinguish%20language%20according%20to%20characters.py\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "https://www.yceffort.kr/2019/01/30/pytorch-3-convolutional-neural-network(2)/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from PIL import Image \n",
    "import numpy as np \n",
    "#pre_img = Image.open(\"./broken_data/broken_data/broken_processed_data/20200519_12_broken_take14.jpg\")\n",
    "#pre_img = Image.open(\"./normal_data/normal_processed_data/20200518_14_normal_take6.jpg\")\n",
    "\n",
    "pre_img = pre_img.convert('L')\n",
    "      \n",
    "# # 이미지를 28, 28로 일괄 리사이즈 \n",
    "resized_pre_img = pre_img.resize((28,28))\n",
    "np_pre_img = np.asarray(resized_pre_img)\n",
    "np_pre_img = np.reshape(np_pre_img, [1,1,28,28])\n",
    "#torch.Size([3056, 1, 28, 28])\n",
    "#type(np_pre_img)\n",
    "\n",
    "pre_X = torch.from_numpy(np_pre_img).float()\n",
    "#pre_X\n",
    "#pre_X.shape\n",
    "\n",
    "result = torch.max(model(pre_X).data, 1)[1]\n",
    "_pre_img = np.reshape(np_pre_img, [1,1,28,28])\n",
    "#torch.Size([3056, 1, 28, 28])\n",
    "#type(np_pre_img)\n",
    "\n",
    "pre_X = torch.from_numpy(np_pre_img).float()\n",
    "#pre_X\n",
    "#pre_X.shape\n",
    "\n",
    "result = torch.max(model(pre_X).data, 1)[1]\n",
    "#print(result)\n",
    "print(\"이 커피콩은 \",categories[result[0]], \"입니다.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitprojectopencvconda6fc4edbc446e42ca82ff567e09c1586a",
   "display_name": "Python 3.7.7 64-bit ('project_opencv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}